---
title: "DATA HARVESTING PROJECT"
format: html
editor: visual
---

## APUNTES

INTRODUCÇAO

In recent years, LEGO sets have evolved from being mere children's toys to highly sought-after investment assets. A study by the Higher School of Economics in Moscow revealed that the value of retired LEGO sets has increased by an average of 11% annually, outperforming many conventional investments since they appreciate in value faster than gold, stocks, or traditional bonds (citar). Factors such as exclusivity, franchise popularity, and set rarity directly influence their resale price, making LEGO an unexpected yet lucrative investment niche.

This project aims to explore the factors driving the revaluation of LEGO sets, analyzing how their prices change over time and identifying which sets offer the greatest return on investment. By examining historical and current market data, we seek to uncover patterns that influence a set’s desirability and long-term worth.

1.  **The official LEGO website**: To obtain the current prices of sets available on the market.

2.  **BrickLink**: A comprehensive online archive that tracks all LEGO sets, their specifications, and their price evolution over time.

The dataset will include essential details such as initial retail price, current market value, percentage appreciation, number of pieces and theme classification.

With the help of statistical analysis and visualizations, we will explore questions such as:

-   Which LEGO sets have appreciated the most over time?

-   Do certain themes, such as *Star Wars* or *Modular Buildings*, have higher investment potential?

-   How do factors like piece count and exclusive minifigures impact resale value?

To obtain and analyze this data, we will implement web scraping techniques using **R** and the **rvest** package, allowing us to track both historical and real-time pricing trends.

Through this research, we aim to uncover patterns that help identify which themes are the most profitable over time, providing valuable insights for both collectors and investors in this emerging market.

To check the childrens of a node. Esto busca el primer child

```{r}
# xml_child returns only one child (specified in search)
# Here, jason is the first child
xml_child(xml_raw, search = 1)
```

Y esto el segundo:

```{r}
# Here, carol is the second child
xml_child(xml_raw, search = 2)
```

Using the `xml_attrs` function we can extract all attributes that match a specific name:

```{r}
person_nodes <- xml_children(child_xml)
```

```{r}
# Extract the attribute type from all nodes
xml_attrs(person_nodes, "type")
```

Aquí te encontraria el nod person ya que es el unico que tiene el atributo *type:*

```         
## <people> ##   <jason> ##     <person [type]> ##       <first_name> ##         <married> ##           {text} ##       <last_name> ##         {text} ##       <occupation> ##         {text} ##   <carol> ##     <person [type]> ##       <first_name> ##         <married> ##           {text} ##       <last_name> ##         {text} ##       <occupation> ##         {text}
```

Using the `xml_path` function you can even find the ‘address’ of these nodes to retrieve specific tags without having to write down `xml_children` many times. For example:

```{r}
xml_path(person_nodes)

# You can use results from xml_path like directories
xml_find_all(xml_raw, "/people/jason/person")
```

`//` is very handy, it means: search the entire document and bring me back all `<dansimmons>` tags:

```{r}
book_xml %>%
  xml_find_all("//dansimmons")
```

What would an XPath expression look like to subset only the 2nd `<book>` tag of `dansimmons`? We can tell XPath the position of the tag we want using `[number]`, where number is replaced with the position:

```{r}
book_xml %>%
  xml_find_all("//dansimmons/book[2]")
```

Path introduces the `*` as a wildcard pattern to return all children of current parent tag.:

```{r}
book_xml %>%
  xml_find_all("//dansimmons/*")
```

Similarly, `*` can be used to fill out a tag which you don’t know the name of. You know that each author has `<book>` tags but you don’t know the name of all authors. You could extract all book tags like this:

```{r}
book_xml %>%
  xml_find_all("/*/*/*/book")
```

Whenever we want our tags to match a specific attribute we can add two brackets at the end of the tag and match the attribute to what we’re after.

```{r}
book_xml %>%
  xml_find_all("//dansimmons//book[@price='yes']") %>%
  xml_text()
```

XPath has all the goodies to perform basic filtering (`and`, `or`, `=`, `!=`) but also has additional functions that are useful for filtering. Some of the most common ones include:

-   [`contains()`](https://tidyselect.r-lib.org/reference/starts_with.html)

-   `starts-with()`

-   [`text()`](https://rdrr.io/r/graphics/text.html)

-   [`not()`](https://magrittr.tidyverse.org/reference/aliases.html)

-   [`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r}
library(rvest)
library(xml2)
library(httr)

url <- "https://www.bricklink.com/catalogTree.asp?itemType=S"

url <- url |> 
  read_html()

url |> 
  xml_find_all("//table[@class='bg-color--white catalog-list__category-list--internal catalog-tree__category-list--internal']//div[contains(text(), 'Tree')]")|> xml_parent() |> xml_parent() |> xml_parent() |> 
  html_table()

```



```{r}
library(stringr)

disney <- "https://www.lego.com/es-es/themes/disney"

disney <- disney |> 
  read_html()

prices <- disney |> 
  xml_find_all("//div[@class='ProductLeaf_priceRow__RUx3P']") |> 
  html_text() 

prices <- gsub(",", ".", prices)
prices <- str_replace(prices, "\\s?€$", "") |> 
  as.numeric()

prices
```

```{r}
titles <- disney |> 
  xml_find_all("//a[@class='ds-body-md-medium ProductLeaf_title__1UhfJ ']") |> 
  xml_children() |> 
  html_text()

titles
```

```{r}
pieces <- disney |> 
  xml_find_all("//span[@data-test='product-leaf-piece-count-label']")|>
  html_text() |> 
  as.numeric()

pieces
```
