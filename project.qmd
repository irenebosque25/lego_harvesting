---
title: "DATA HARVESTING PROJECT"
author: Irene Bosque and Pablo Aísa
format: html
editor: visual
---

# Introduction

In recent years, LEGO sets have evolved from being mere children's toys to highly sought-after investment assets. A study by the Higher School of Economics in Moscow revealed that the value of retired LEGO sets has increased by an average of 11% annually, outperforming many conventional investments since they appreciate in value faster than gold, stocks, or traditional bonds (citar). Factors such as exclusivity, franchise popularity, and set rarity directly influence their resale price, making LEGO an unexpected yet lucrative investment niche.

This project aims to explore the factors driving the revaluation of LEGO sets, analyzing how their prices change over time and identifying which sets offer the greatest return on investment. By examining historical and current market data, we seek to uncover patterns that influence a set’s desirability and long-term worth.

1.  **The official LEGO website**: To obtain the current prices of sets available on the market.

2.  **BrickLink**: A comprehensive online archive that tracks all LEGO sets, their specifications, and their price evolution over time.

The dataset will include essential details such as initial retail price, current market value, percentage appreciation, number of pieces and theme classification.

With the help of statistical analysis and visualizations, we will explore questions such as:

-   Which LEGO sets have appreciated the most over time?

-   Do certain themes, such as *Star Wars* or *Modular Buildings*, have higher investment potential?

-   How do factors like piece count and exclusive minifigures impact resale value?

To obtain and analyze this data, we will implement web scraping techniques using **R** and the **rvest** package, allowing us to track both historical and real-time pricing trends.

Through this research, we aim to uncover patterns that help identify which themes are the most profitable over time, providing valuable insights for both collectors and investors in this emerging market.

## Libraries

```{r}
library(rvest)
library(xml2)
library(httr)
library(stringr)
library(tidyverse)
library(tibble)
library(readr)
```

# Bricklink

We begin the web scraping process for BrickLink, aiming to create a structured dataset containing all the products to be analyzed. Our objective is to extract key variables, including price, release year, and franchise affiliation.

To achieve this, we need to scrape four different hyperlinks within the same website:

Main Category Page: The first hyperlink leads to a table listing all LEGO categories available on BrickLink. This includes the franchises we want to compare, such as Star Wars, Super Mario, and others.

Category-Specific Product Listings: Clicking on any franchise category takes us to a second hyperlink, which displays all products belonging to that category. This page provides details such as product names, associated LEGO sets, and the number of pieces each product contains.

Individual Product Details: By selecting a specific product, we are redirected to a third hyperlink that contains additional information, including the year the product was released. On this page, we also find an option labeled "View Price Guide", which directs us to the final crucial dataset.

Price Guide – Current Listings: The fourth hyperlink leads to the "Current Items for Sale: New" table, which is particularly important for our analysis. This table contains real-time market data on new products currently available, including variables such as the total quantity of sales of this product, its average price and its maximum price

To construct our final dataset, we need to sequentially scrape each of these four hyperlinks. We will develop a function that systematically navigates through each of them, extracting relevant data at each stage until we obtain the complete dataset.

## Categories selected

With these in mind, the first step in this process is to create a table with all categories available in the web and each link that leads to the second hyperlink with the Category-Specific Product Listings. We, then, filter for the specific categories we are interested in:

```{r}

brick <- "https://www.bricklink.com/catalogTree.asp?itemType=S"
brick <- read_html(brick)

# All the links inside the main page
links <- brick |>
  html_nodes("a") |> 
  html_attr("href")

# Links of the main categories
cat_links <- 
  links[grepl("catalogList.asp\\?catType=S&catString=[0-9]+$", links)]

# Names of the main categories
cat_names <- brick |> 
  html_nodes("a b") |> 
  html_text()

cat_links <- paste0("https://www.bricklink.com", cat_links)

# Data.frame with all the info
main_categories <- data.frame(
  category_name = cat_names,
  category_url = cat_links)
```

Now we select only the variables that we are interested in.

```{r}
all_products <- main_categories |> 
  filter(category_name %in% c("Town", "Disney", "Harry Potter", 
                              "Jurassic Park", 
                              "The Hobbit and The Lord of the Rings", 
                            "Super Heroes", "Star Wars", "Super Mario"))
```

Then, we get the number of pages that every category has and we add it to the main_categeories table . First we try for one single category (Star Wars in this case) to know which is the procedure to get the number of pages and then we create the function to apply it to the other categories more later:

```{r}
# First try only with Star Wars sets

sw_link <- main_categories |> 
  filter(category_name == "Star Wars") |> 
  select(category_url) |> 
  as.character() 

sw <- sw_link |> 
  read_html()

sw_pages <- sw |> 
  html_nodes(xpath ="//div[2]/div[2]/b[3]") |> 
  html_text() |> 
  as.numeric()
sw_pages
```

It can be seen that the number of pages indicated for Star Wars is 20, which is correct as this is the number of pages containing objects in this category on the website. The next step is to automate this process and apply it to the rest of the selected categories.

```{r}
# Function to obtain the number of pages inside each category
sets_pages <- function(url) {
  link <- read_html(url)
  
  pages <- link |> 
    html_nodes(xpath = "//div[2]/div[2]/b[3]") |> 
    html_text() |> 
    as.numeric()
  
  if (length(pages) == 0 || is.na(pages)) {
    pages <- 1
  }
  
  Sys.sleep(3)
  
  return(pages)
}

main_categories <- all_products |> 
  mutate(n_pages = sapply(category_url, sets_pages))
```

## Product data

### First try products of Star Wars

Then, we get the number of pages that every category has and we add it to the main_categeories table . First we try for one single category (Star Wars in this case) to know which is the procedure to get the number of pages and then we create the function to apply it to the other categories more later:

```{r}
# Link vector
link <- "https://www.bricklink.com/catalogList.asp?catType=S&catString=65" |>
  read_html()

raw_links <- link |> 
  xml_find_all("//div[@class='container-xl container-body l-pad-y l-margin-bottom catalog-list__body']//a") |> 
  html_attr("href")

# Filter only the ones that contain sets
set_links <- raw_links[str_detect(raw_links, "/v2/catalog/catalogitem.page\\?S=")]

base_url <- "https://www.bricklink.com"  
full_links <- paste0(base_url, set_links)
```

Now we get the names of every product of the Star Wars category:

```{r}
# Names vector
names <- link |>
  xml_find_all("//table[@class='bg-color--white catalog-list__body-main catalog-list__body-main--alternate-row']//strong")|> html_text() 
```

Then we get the number of pieces each product has:

```{r}
year_pieces <- link |> 
  xml_find_all("//table[@class='bg-color--white catalog-list__body-main catalog-list__body-main--alternate-row']//font[@class='fv']") |> 
  html_text()

extract_info <- function(entry) {
  pieces <- str_extract(entry, "\\d+ Parts") |> 
    str_remove(" Parts")
  set <- str_extract(entry, "(?<=Catalog: Sets:).*")
  
  return(tibble(Pieces = as.integer(pieces), Set = set))
}
```

Finally, we create the function that generalizes all the previous steps in order to get a table with all the information: name, piece and the link redirects us to each product link.

```{r}
# Function to scrap the products and obtain the info
scrape_products <- function(base_url, total_pages) {
  
  # URL with pages
  paged_url <- paste0(base_url, "&pg=%d&v=1")  # Add more pages
  
  all_names <- c()
  all_links <- c()
  all_pieces <- c()
  all_sets <- c()
  
  for (page in 1:total_pages) {
    
    if (page == 1) {
      page_url <- base_url # First page
    } else {
      page_url <- sprintf(paged_url, page)  # the rest
    }
    
    link <- read_html(page_url)
    
    # Links
    raw_links <- link |> 
      xml_find_all("//div[@class='container-xl container-body l-pad-y l-margin-bottom catalog-list__body']//a") |> 
      html_attr("href")
    
    set_links <- raw_links[str_detect(raw_links, "/v2/catalog/catalogitem.page\\?S=")]
    full_links <- paste0("https://www.bricklink.com", set_links)
    
    # Names
    names <- link |> 
      xml_find_all("//table[@class='bg-color--white catalog-list__body-main catalog-list__body-main--alternate-row']//strong") |> 
      html_text()
    
    # More info
    raw_data <- link |> 
      xml_find_all("//table[@class='bg-color--white catalog-list__body-main catalog-list__body-main--alternate-row']//font[@class='fv']") |> 
      html_text()
    
    cleaned_data <- map_dfr(raw_data, extract_info)
    
    all_names <- c(all_names, names)
    all_links <- c(all_links, full_links)
    all_pieces <- c(all_pieces, cleaned_data$Pieces)
    all_sets <- c(all_sets, cleaned_data$Set)
  }
  Sys.sleep(2)
  
  # Final df
  tibble(Name = all_names, Link = all_links, 
         Pieces = all_pieces, Set = all_sets)
}

# Try with Star Wars
brick_starwars <- scrape_products("https://www.bricklink.com/catalogList.asp?catType=S&catString=65", total_pages = 20)
```

The next step consists on applying the function to the rest of the categories, so we have a complete dataset of all the products that we will be analyzing.

```{r}
# The rest of the categories
all_products <- main_categories |> 
  mutate(product_data = 
           pmap(list(category_url, n_pages), scrape_products)) |> 
  unnest(cols = c(product_data)) |> 
  select(-c(category_url, n_pages))

colSums(is.na(all_products))
```

As it can be seen, there are 402 sets that have missing values for the number of pieces. This is not an error of scraping, this is due to the fact that these products are sets of Legos that include several products in one offer. For this reason, we prefer to drop these missing cases. This would also decrease the loading time of future operations.

```{r}
all_products <- all_products |> 
  drop_na(Pieces)
```

Now we have all the main information for the products organized in one table. However, to proceed with further analysis, we will have to add more information about these products.

### Year

```{r}
# To obtain the year of the product
link_product |> 
  xml_find_all("//a[@class='links']") |> 
  html_text() |> 
  head(1)
```

```{r}
scrape_years <- function(df) {
  df <- df %>%
    mutate(year = map_chr(Link, function(product_url) {
      page <- tryCatch(read_html(product_url), error = function(e) return(NA))
      
      if (is.na(page)) return(NA)
      
      # Extract the year (first link in that class)
      product_year <- page %>%
        html_elements("a.links") %>%
        html_text(trim = TRUE) %>%
        head(1)

      return(product_year)
    }))
  
  return(df)
}
```

```{r}
all_products <- scrape_years(all_products)
head(all_products)
```

### Product prices

As we did in the first part of the scraping process, we try to scrap first for one single product and more later we will make a function to generalize it to every product.

The first thing we do in this step is to get the final hyperlynk for this specific product ,the one that has all the information about its prices.

```{r}
link_product <- "https://www.bricklink.com/v2/catalog/catalogitem.page?S=3219-1#T=S&O={%22iconly%22:0}" |> 
  read_html()

link_product |> 
      xml_find_all("//div[@id='_idPriceGuideLink']//a") |> 
      html_attr("href")
```

Now that we got the final hyperlink, we extract the table with all the current items and prices for sale

```{r}
link_historial <- 
  "https://www.bricklink.com/catalogPG.asp?S=3219-1&ColorID=0"|> 
  read_html()

link_historial |> 
  xml_find_all("//table[@class='fv']//td[@valign='TOP']") |> 
  html_text()

current_items <- link_historial |> 
  xml_find_all("(//table[@class='fv']//td[@valign='TOP'])[3]") |> # the number 3 indicates the number of the table that we want. In this case, current items for sale.
  html_text()

current_items
```

Then we try to clean it and put every price information in separate columns

```{r}
# Titles and values
titles <- unlist(str_extract_all(current_items, "[A-Za-z ]+(?=:)"))
values <- unlist(str_extract_all(current_items, "(?<=:)\\s*[A-Z]*\\s*[0-9.,]+"))

# Extract the currency
currency <- unique(na.omit(str_extract(values, "^[A-Z]+")))[1]

# Numerical values
numeric_values <- str_extract(values, "[0-9,.]+")

# Tibble with the data extracted
final_result <- tibble(!!!set_names(numeric_values, titles)) |> 
  mutate(Currency = currency) # column for the currency

final_result
```

### Data guide history

#### Getting the guides links

```{r}
library(rvest)

get_price_history_links <- function(product_link) {
  # Extract the code of the set from the URL product
  product_code <- str_extract(product_link, "S=[A-Za-z0-9-]+")
  if (is.na(product_code)) {
    return(NA)
  }
  
  # Verify if the link already contains "https://"
  price_guide_link <- sprintf("https://www.bricklink.com/catalogPG.asp?%s&ColorID=0", product_code)
  
  # If not, it is not necessary to add it
  if (grepl("^https://", price_guide_link)) {
    return(price_guide_link)
  }
  Sys.sleep(3)
  
  # Paste if necessary
  return(paste0("https://www.bricklink.com", price_guide_link))
}

```

```{r}
all_products <- all_products |> 
  mutate(price_history_link = map(Link, get_price_history_links))
```

Extraemos todos los links del history price:

```{r}
extraer_link <- function(url) {
  # Intentamos leer el HTML de la página
  link_product <- tryCatch({
    read_html(url)
  }, error = function(e) {
    # Si hay un error al leer la página, retornamos NA
    return(NULL)
  })
  
  # Si no conseguimos leer la página correctamente, retornamos NA
  if (is.null(link_product)) {
    return(NA)
  }
  
  # Extraemos el enlace que buscamos
  resultado <- link_product |> 
    xml_find_all("//div[@id='_idPriceGuideLink']//a") |> 
    html_attr("href")
  
  # Si no se encuentra el enlace, retornamos NA
  if (length(resultado) == 0) {
    return(NA)
  }
  Sys.sleep(3)
  
  return(resultado)
}

# Aplicamos la función a cada enlace en la columna 'Link' de todo el dataset
all_products2 <- all_products %>%
  mutate(price_link = purrr::map_chr(Link, ~ {
    Sys.sleep(2)  # Pausa de 2 segundos entre cada solicitud
    extraer_link(.)
  }))
```

Y aqui extraemos el dataset paraa no tener que volver a correr la funcion: 
```{r}
all_products2 <- read_csv("all_products2.csv")

colSums(is.na(all_products2))

all_products_clean <- all_products2 |> 
  drop_na(price_link) # products that were never sold
```

#### Data prices

```{r}
# URL de ejemplo (puedes reemplazarla con el link que tengas)
url_guide <- "https://www.bricklink.com/catalogPG.asp?S=4493-1&ColorID=0"

# Leer la página
page_guide <- read_html(url_guide)

```

```{r}
# Extraer el "Times Sold"
times_sold <- page_guide %>% 
  xml_find_first("//table[@class='fv']//tr[1]//td[2]/b") %>% 
  html_text(trim = TRUE)

# Extraer el "Total Qty"
total_qty <- page_guide %>%
  xml_find_first("//table[@class='fv']//tr[2]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer el "Min Price"
min_price <- page_guide %>%
  xml_find_first("//table[@class='fv']//tr[3]/td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer el "Avg Price"
avg_price <- page_guide %>%
  xml_find_first("//table[@class='fv']//tr[4]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer el "Qty Avg Price"
qty_avg_price <- page_guide %>%
  xml_find_first("//table[@class='fv']//tr[5]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer el "Max Price"
max_price <- page_guide %>%
  xml_find_first("//table[@class='fv']//tr[6]//td[2]/b") %>%
  html_text(trim = TRUE)

```

```{r}
# Crear un tibble con los resultados
results_guide <- tibble(
  Times_Sold = times_sold,
  Total_Qty = total_qty,
  Min_Price = min_price,
  Avg_Price = avg_price,
  Qty_Avg_Price = qty_avg_price,
  Max_Price = max_price
)

results_guide
```

Before moving to the function process, let's try with another product to see if we obtain all the data correctly.

```{r}
url_guide2 <- "https://www.bricklink.com/catalogPG.asp?S=30162-1&ColorID=0"  # Cambia este enlace por el que necesites

# Leer la página
page_guide2 <- read_html(url_guide2)

# 2. Extraer los datos de la página utilizando los XPaths

# Extraer Times Sold
times_sold <- page_guide2 %>%
  xml_find_first("//table[@class='fv']//tr[1]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer Total Qty
total_qty <- page_guide2 %>%
  xml_find_first("//table[@class='fv']//tr[2]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer Min Price
min_price <- page_guide2 %>%
  xml_find_first("//table[@class='fv']//tr[3]/td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer Avg Price
avg_price <- page_guide2 %>%
  xml_find_first("//table[@class='fv']//tr[4]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer Qty Avg Price
qty_avg_price <- page_guide2 %>%
  xml_find_first("//table[@class='fv']//tr[5]//td[2]/b") %>%
  html_text(trim = TRUE)

# Extraer Max Price
max_price <- page_guide2 %>%
  xml_find_first("//table[@class='fv']//tr[6]//td[2]/b") %>%
  html_text(trim = TRUE)

# 3. Crear un tibble con los datos extraídos
price_data2 <- tibble(
  Times_Sold = times_sold,
  Total_Qty = total_qty,
  Min_Price = min_price,
  Avg_Price = avg_price,
  Qty_Avg_Price = qty_avg_price,
  Max_Price = max_price
)
```

#### Prices guide function

```{r}
# Contador global de productos procesados
counter <- 0

# Función modificada para manejar las pausas y guardar los datos en CSV
history_data <- function(price_link) {
  # Leer la página de historial de precios
  price_history_page <- tryCatch(read_html(price_link), 
                                 error = function(e) return(NULL))
  
  # Si no se puede leer la página, devolver valores NA
  if (is.null(price_history_page)) {
    return(tibble(Times_Sold = NA, Total_Qty = NA, Min_Price = NA, 
                  Avg_Price = NA, Qty_Avg_Price = NA, Max_Price = NA))
  }

  # Extraer los datos utilizando XPaths específicos
  data <- tibble(
    Times_Sold = price_history_page %>%
      xml_find_first("//table[@class='fv']//tr[1]//td[2]/b") %>%
      html_text(trim = TRUE),
    
    Total_Qty = price_history_page %>%
      xml_find_first("//table[@class='fv']//tr[2]//td[2]/b") %>%
      html_text(trim = TRUE),
    
    Min_Price = price_history_page %>%
      xml_find_first("//table[@class='fv']//tr[3]/td[2]/b") %>%
      html_text(trim = TRUE),
    
    Avg_Price = price_history_page %>%
      xml_find_first("//table[@class='fv']//tr[4]//td[2]/b") %>%
      html_text(trim = TRUE),
    
    Qty_Avg_Price = price_history_page %>%
      xml_find_first("//table[@class='fv']//tr[5]//td[2]/b") %>%
      html_text(trim = TRUE),
    
    Max_Price = price_history_page %>%
      xml_find_first("//table[@class='fv']//tr[6]//td[2]/b") %>%
      html_text(trim = TRUE)
  )
  
  # Incrementar el contador de productos
  counter <<- counter + 1
  
  # Después de cada 5 productos procesados, guardar los datos en CSV
  if (counter %% 7 == 0) {
    Sys.sleep(5)  # Pausa larga después de cada 5 productos
  } else {
    Sys.sleep(runif(1, min = 3, max = 5))  # Pausa normal entre productos
  }
  
  return(data)
}

```

```{r}
first_try <- all_products_clean[1:1000,]

products1 <- first_try %>%
  mutate(history_data = map(price_link, history_data)) 

products1 <- products1 |> 
  unnest(history_data)
```

