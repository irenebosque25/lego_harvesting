---
title: "copia project"
format: html
editor: visual
---

## APUNTES

INTRODUCCIÓN

In recent years, LEGO sets have evolved from being mere children's toys to highly sought-after investment assets. A study by the Higher School of Economics in Moscow revealed that the value of retired LEGO sets has increased by an average of 11% annually, outperforming many conventional investments since they appreciate in value faster than gold, stocks, or traditional bonds (citar). Factors such as exclusivity, franchise popularity, and set rarity directly influence their resale price, making LEGO an unexpected yet lucrative investment niche.

This project aims to explore the factors driving the revaluation of LEGO sets, analyzing how their prices change over time and identifying which sets offer the greatest return on investment. By examining historical and current market data, we seek to uncover patterns that influence a set’s desirability and long-term worth.

1.  **The official LEGO website**: To obtain the current prices of sets available on the market.

2.  **BrickLink**: A comprehensive online archive that tracks all LEGO sets, their specifications, and their price evolution over time.

The dataset will include essential details such as initial retail price, current market value, percentage appreciation, number of pieces and theme classification.

With the help of statistical analysis and visualizations, we will explore questions such as:

-   Which LEGO sets have appreciated the most over time?

-   Do certain themes, such as *Star Wars* or *Modular Buildings*, have higher investment potential?

-   How do factors like piece count and exclusive minifigures impact resale value?

To obtain and analyze this data, we will implement web scraping techniques using **R** and the **rvest** package, allowing us to track both historical and real-time pricing trends.

Through this research, we aim to uncover patterns that help identify which themes are the most profitable over time, providing valuable insights for both collectors and investors in this emerging market.

```{r}
library(stringr)
library(rvest)
library(tidyverse)
library(tibble)
library(xml2)

disney <- "https://www.lego.com/es-es/themes/disney"

disney <- disney |> 
  read_html()

prices <- disney |> 
  xml_find_all("//div[@class='ProductLeaf_priceRow__RUx3P']") |> 
  html_text() 

prices <- gsub(",", ".", prices)
prices <- str_replace(prices, "\\s?€$", "") |> 
  as.numeric()

prices
```

```{r}
titles <- disney |> 
  xml_find_all("//a[@class='ds-body-md-medium ProductLeaf_title__1UhfJ ']") |> 
  xml_children() |> 
  html_text()

titles
```

```{r}
pieces <- disney |> 
  xml_find_all("//span[@data-test='product-leaf-piece-count-label']")|>
  html_text() |> 
  as.numeric()

pieces
```
```{r}
product <- disney |> 
  xml_find_all("//article[@class='ProductLeaf_wrapper__H0TCb ']") 

product
```


```{r}

extract_lego_data <- function(base_url, max_pages = 4) {
  
  lego_list <- list()  # Lista para almacenar cada página
  
  for (page_num in 1:max_pages) {
    page_url <- paste0(base_url, "?page=", page_num, "&offset=0")
    page <- read_html(page_url)
    
    # Extraer los bloques de cada producto
    product_nodes <- page |> 
    xml_find_all("//article[@class='ProductLeaf_wrapper__H0TCb ']")  # Asegurar que sean los mismos bloques
    
    # Extraer la información dentro de cada bloque
    titles <- product_nodes |> 
      xml_find_first(".//a[contains(@class, 'ProductLeaf_title')]") |> 
      html_text()
    
    prices <- product_nodes |> 
      xml_find_all("//div[@class='ProductLeaf_priceRow__RUx3P']") |> 
      html_text() 
    
    
    # Eliminar comas, espacios y símbolos de euro
    prices <- gsub(",", ".", prices)
    prices <- str_replace(prices, "\\s?€$", "") |> 
      as.numeric()
    
    
    pieces <- product_nodes |> 
      xml_find_first(".//span[@data-test='product-leaf-piece-count-label']") |> 
      html_text() |> 
      as.numeric()
    
    # Crear un dataframe temporal
    lego_page <- tibble(
      Title = titles,
      Price = prices,
      PieceCount = pieces
    )
    
    # Guardar la página en la lista
    lego_list[[page_num]] <- lego_page
    Sys.sleep(2)
  }
  
  # Combinar todas las páginas en un solo data frame
  lego_data <- bind_rows(lego_list)
  
  return(lego_data)
}



```

```{r}
# Disney
url_disney <- "https://www.lego.com/es-es/themes/disney"
lego_disney <- extract_lego_data(url_disney, max_pages = 4) |> 
  mutate("Franchise"= "Disney")
```

```{r}
#Harry Potter
url_harryp <- "https://www.lego.com/es-es/themes/harry-potter"
lego_harryp <- extract_lego_data(url_harryp, max_pages = 4)|> 
  mutate("Franchise"= "Harry Potter")
```

```{r}
#Star Wars
url_starwars <- "https://www.lego.com/es-es/themes/star-wars"
lego_starwars <- extract_lego_data(url_starwars, max_pages = 6)|> 
  mutate("Franchise"= "Star Wars")
```

```{r}
#Super Mario
url_supermario <- "https://www.lego.com/es-es/themes/super-mario"
lego_supermario <- extract_lego_data(url_supermario, max_pages = 2)|> 
  mutate("Franchise"= "Super Mario")
```

```{r}
#Lord of the Rings
url_rings<- "https://www.lego.com/es-es/themes/lord-of-the-rings"
lego_lotr <- extract_lego_data(url_rings, max_pages = 2)|> 
  mutate("Franchise"= "Lord of the Rings")
```

```{r}
#Marvel
url_marvel <- "https://www.lego.com/es-es/themes/marvel"
lego_marvel <- extract_lego_data(url_marvel, max_pages = 4)|> 
  mutate("Franchise"= "Marvel")
```

```{r}
#City
url_city <-"https://www.lego.com/es-es/themes/city"
lego_city <- extract_lego_data(url_city, max_pages = 5)|> 
  mutate("Franchise"= "City")
```

```{r}
url_jurassic <- "https://www.lego.com/es-es/themes/jurassic-world"
lego_jurassic <- extract_lego_data(url_jurassic, max_pages = 1) |> 
  mutate("Franchise"= "Jurassic World")
```

Las unimos todas:

```{r}
library(tidyverse)
official_lego <- bind_rows(lego_disney, lego_city, lego_harryp, lego_jurassic, lego_lotr, lego_marvel, lego_starwars, lego_supermario)
```

Eliminamos los NA de piezas ya que no son sets de lego si no otros productos que vende la marca como peluches, boligrafos, libretas.... que no nos interesan para nuestro estudio

```{r}
official_lego <- official_lego |> 
  drop_na (PieceCount)
```

# BrickLink

```{r}
library(rvest)
library(xml2)
library(httr)
library(stringr)

url<- "https://www.bricklink.com/catalogTree.asp?itemType=S"

lego <- read_html(url)

# All the links inside the main page
links <- lego |>
  html_nodes("a") |> 
  html_attr("href")

# Links of the main sections
section_links <- 
  links[grepl("catalogList.asp\\?catType=S&catString=[0-9]+$", links)]

# Names of the main sections
section_names <- lego |> 
  html_nodes("a b") |> 
  html_text()

section_links <- paste0("https://www.bricklink.com", section_links)

# Data.frame with all the info
main_sections <- data.frame(
  section_name = section_names,
  section_url = section_links)

```

Intentamos scrapear primero para Star Wars y luego ya normalizamos para el resto de franquicias:

```{r}
#vector con los links
link <- "https://www.bricklink.com/catalogList.asp?catType=S&catString=65" |> 
  read_html()

raw_links <- link |> 
  xml_find_all("//div[@class='container-xl container-body l-pad-y l-margin-bottom catalog-list__body']//a") |> 
  html_attr("href")

# Filtrar solo los que llevan a sets (parecen tener "/v2/catalog/catalogitem.page?S=")
set_links <- raw_links[str_detect(raw_links, "/v2/catalog/catalogitem.page\\?S=")]

base_url <- "https://www.bricklink.com"  
full_links <- paste0(base_url, set_links)
```

```{r}
#vector con los nombres
names <- link |>
  xml_find_all("//table[@class='bg-color--white catalog-list__body-main catalog-list__body-main--alternate-row']//strong") |> html_text() 
  
```

```{r}
brick_starwars <- tibble(
  Name = names,
  Link = full_links
)
```

Ahora necesitamos hacer un bucle que recorra todas las paginas de starwars ya que hay 20, y esto solo corre la primera de todas y por lo tanto solo los 50 primeros productos:

```{r}
# Definir la función
scrape_products <- function(base_url, total_pages) {
  
  # Crear la URL paginada
  paged_url <- paste0(base_url, "&pg=%d&v=1")  # Agrega paginación
  
  # Inicializar vectores vacíos
  all_names <- c()
  all_links <- c()
  
  # Recorrer todas las páginas
  for (page in 1:total_pages) {
    
    # Determinar la URL de la página actual
    if (page == 1) {
      page_url <- base_url  # Primera página usa la URL base
    } else {
      page_url <- sprintf(paged_url, page)  # Otras páginas usan la URL paginada
    }
    
    # Leer la página web
    link <- read_html(page_url)
    
    # Extraer los links de los sets
    raw_links <- link |> 
      xml_find_all("//div[@class='container-xl container-body l-pad-y l-margin-bottom catalog-list__body']//a") |> 
      html_attr("href")
    
    set_links <- raw_links[str_detect(raw_links, "/v2/catalog/catalogitem.page\\?S=")]
    full_links <- paste0("https://www.bricklink.com", set_links)
    
    # Extraer los nombres
    names <- link |> 
      xml_find_all("//table[@class='bg-color--white catalog-list__body-main catalog-list__body-main--alternate-row']//strong") |> 
      html_text()
    
    # Guardar en vectores
    all_names <- c(all_names, names)
    all_links <- c(all_links, full_links)
  }
  
  # Crear el dataframe final
  tibble(Name = all_names, Link = all_links)
}

# USO: Scrapear Star Wars (solo cambiando la URL)
brick_starwars <- scrape_products("https://www.bricklink.com/catalogList.asp?catType=S&catString=65", total_pages = 20)

```

Ya tenemos los 974 productos de la francicia de StarWars. Ahora hay que normalizarlo de todos los links que seria coger el link de la franquicia y cambiarlo en scrape_products, y cambiar tambien manualmente el numero de paginas que tenga. Lo probamos con Harry Potter:

```{r}
main_sections |> 
  filter(section_name=="Harry Potter")
```

```{r}
brick_harrypotter <- scrape_products("https://www.bricklink.com/catalogList.asp?catType=S&catString=227", total_pages = 4)	
```

va genial que ilusion pablo!!!!!!!

Lo que hay que intentar luego es automatizar todo el proceso que de la primera pagina de links haga el segundo proceso y el tercero y asi sabes??? pero bueno lo intentamos al final.

Ahora intentamos acceder al link de el historial de precios a partir de un producto en concreto(cogido de la lista de links anteriores), en este caso TIE Fighter - Mini polybag de la franquicia Star Wars:

```{r}
link_producto <- "https://www.bricklink.com/v2/catalog/catalogitem.page?S=3219-1#T=S&O={%22iconly%22:0}" |> 
  read_html()

link_producto |> 
      xml_find_all("//div[@id='_idPriceGuideLink']//a") |> 
      html_attr("href")
```

oye yo no entiendo porque me esta saliendo todo tan bien la vida es maravillosa

Ahora dentro del historial de precios del producto necesitamos substraer la tabla Current Items for Sale: New (no??? es ese el que queremos verdad) lo que este en concreto esta en ROL y no se que divisa es esa la vd:

```{r}
link_historial <- "https://www.bricklink.com/catalogPG.asp?S=3219-1&ColorID=0" |> 
  read_html()


link_historial |> 
  xml_find_all("//table[@class='fv']//td[@valign='TOP']") |> 
  html_text()

current_items <- link_historial |> 
  xml_find_all("(//table[@class='fv']//td[@valign='TOP'])[3]") |> #si luego cambiamos de opinion y queremos coger otra tabla como used, last month sales o lo que sea cambiamos el numero 3 y ya esta, ahora solo he cogido la tabla: Current Items for Sale: New
  html_text()

current_items
```

Ahora la limpiamos:

```{r}
# Expresión regular para extraer los títulos y los valores
titles <- unlist(str_extract_all(current_items, "[A-Za-z ]+(?=:)"))

# Extraer valores después de los dos puntos
values <- unlist(str_extract_all(current_items, "(?<=:)\\s*[A-Z]*\\s*[0-9.,]+"))


# Crear un tibble con los datos limpios
table_current_items <- tibble(
  Title = str_trim(titles), 
  Value = str_trim(values)   
)

# Separar moneda y número si existe una divisa
table_current_items <- table_current_items %>%
  mutate(Currency = str_extract(Value, "^[A-Z]+"), 
         Value = str_extract(Value, "[0-9,.]+"))  

table_current_items
```
